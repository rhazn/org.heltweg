[{"content":"Overview I am a big fan of separating a web client (typically as a single page application) from a backend that can be accessed using a REST api. The setup feels intuitive for me and can easily be extended for branching out into different clients, like mobile apps.\nFor development, I package everything as simple as I can using docker and docker compose. Docker compose is a nice middle ground that is easier to set up and run than kubernetes but means you are already \u0026ldquo;halfway there\u0026rdquo; if you want to deploy into a k8s cluster later on.\nFor Wanderlist, I wanted to try out some new tech that I had heard or read about but never used: TailwindCSS for the client and Pocketbase for the backend.\nPocketbase: The young gun Pocketbase is a very new open source \u0026ldquo;backend for your next SaaS and Mobile app\u0026rdquo;. What makes it special is that it is just one self-contained file. Data is stored in a local SQLite database, uploads in the local file system or a S3 compatible store.\nI have gotten to really enjoy working with SQLite, either during developing our open-source data pipeline modeling language Jayvee or in the JValue project.\nPocketbase backend After a decade of wrestling complex backend setups or terrible serverless experiences like Firebase, I am excited to work with some simpler tools. And if I manage to outgrow SQLite, I am hyped. I\u0026rsquo;ll write more about my experiences with pocketbase down the line.\nReact with TailwindCSS: Pretty fly for an old man Much has been written about which frontend framework is the best, I just like React. Fun fact, did you know it was open-sourced ten years ago in 2013? Time flies.\nFor the frontend styling, I wanted to try out TailwindCSS. I am not sure how much I like the inline definition of styles but so far it has been weirdly productive. The code itself looks terrible, but the guideline seems to be to refactor any component you use multiple times into a reusable react component. Makes sense to me, we\u0026rsquo;ll see.\nFor icons, I\u0026rsquo;ve found and enjoyed Feather Icons, I think they look great.\nScaffolded client with coming soon page Not much to see there yet, so far, so good.\nDeploying a react single page application with pocketbase using docker compose: Group hug Building and deployment of the whole app (both the SPA using react and the backed with pocketbase) is done using two simple files: A dockerfile and the docker compose file.\nThe dockerfile:\nbuilds the react frontend downloads and extracts pocketbase moves the built frontend assets into a folder that pocketbase serves starts the pocketbase server FROM node:lts-alpine3.18 AS build-web COPY client-web client-web WORKDIR client-web RUN npm ci RUN npm run build FROM node:lts-alpine3.18 ARG PB_VERSION=0.18.2 RUN apk add --no-cache \\ unzip \\ ca-certificates # download and unzip PocketBase ADD https://github.com/pocketbase/pocketbase/releases/download/v${PB_VERSION}/pocketbase_${PB_VERSION}_linux_amd64.zip /tmp/pb.zip RUN unzip /tmp/pb.zip -d /pb/ COPY --from=build-web client-web/dist /pb/pb_public EXPOSE 8080 # start PocketBase CMD [\u0026#34;/pb/pocketbase\u0026#34;, \u0026#34;serve\u0026#34;, \u0026#34;--http=0.0.0.0:8080\u0026#34;] Finally, using docker compose the previously built docker image is instantiated as a service and exposed on port 80. To make sure no data is lost when restarting the service, a volume is defined that I can map to whereever I want on deployment.\nversion: \u0026#34;3.9\u0026#34; services: wanderlist: build: . ports: - \u0026#34;80:8080\u0026#34; volumes: - pocketbase_data:/pb/pb_data volumes: pocketbase_data: Up next User sign in / log in Read the series Project Wanderlist\n","permalink":"https://heltweg.org/projects/wanderlist/setup/","summary":"Overview I am a big fan of separating a web client (typically as a single page application) from a backend that can be accessed using a REST api. The setup feels intuitive for me and can easily be extended for branching out into different clients, like mobile apps.\nFor development, I package everything as simple as I can using docker and docker compose. Docker compose is a nice middle ground that is easier to set up and run than kubernetes but means you are already \u0026ldquo;halfway there\u0026rdquo; if you want to deploy into a k8s cluster later on.","title":"Wanderlist - Setup"},{"content":"Wanderlist One of my favorite ways to explore a new city are free walking tours in which a local guide takes you around the city, shows you interesting places and shares their knowledge with you. At the end of these tours, guides often share their favorite spots and \u0026lsquo;secret tips\u0026rsquo; with you, usually as a way to keep contact or get you to follow their Instagram. We\u0026rsquo;d take these recommendations and, if there is time, plan some of our travel days around visiting them, eating there or whatever else makes sense.\nI always thought, sharing local recommendations would be a cool idea for an app. You gather curated places from friends and locals and have an easy way to plan short trips centered around them. When a lazy Sunday is coming up, just open your app and plan (or let it generate) an interesting afternoon trip to some local point of interest and finishing in a good restaurant.\nI am sure there are a million apps that do this, let\u0026rsquo;s add one more.\nPlanning With these thoughts in mind, I sat down for some sketches with GoodNotes on my iPad. This is currently my favorite way for very low fidelity mockups, basically just a way to quickly get my initial ideas \u0026lsquo;on paper\u0026rsquo;. I\u0026rsquo;ve done some follow up mockups in Excalidraw and the overkill-solution I am aware of is of course Figma. If anyone has one suggestions for good mockup tools, I\u0026rsquo;d be delighted to hear about them.\nWe will need some form of navigation, which will also define the first functionality we want to build. If done well, I think a bit of gamification by collecting points of interests would be nice. So I added a tab to manage your point of interests and a tab to manage \u0026lsquo;wanderlists\u0026rsquo; (basically lists of points of interests to combine into a trip). Then we just need some way to create a new wanderlist and some stats and account management views for good measure.\nScreen 1: The initial navigation and planning a new Wanderlist For every screen, we just need some high-level idea of what it is about. Both managing points of interests and wanderlists are just lists with additional filters. For the points of interests, we also need a way to find new ones. Initially, I assume this will be adding a code from someone to get their list of local suggestions, maybe later on this could be scanning QR codes or opening some random chests (I have been damaged by playing mobile games).\nScreen 2: Point of interest and Wanderlist management Finally, let us add the boring screens we can actually start working on. We will start by creating a skeleton app with a coming soon screen and some user log-in. Because \u0026lsquo;As a user, I want to be able to log in so that I can use the app.\u0026rsquo; is the first story (and lie) every product owner writes to start a new product.\nScreen 3: Coming soon screen and account management Up next Next up, we need to actually write some scaffolding code to get the app running and show some coming soon screens :).\nNext post: Setup\nRead the series Project Wanderlist\n","permalink":"https://heltweg.org/projects/wanderlist/planning/","summary":"Wanderlist One of my favorite ways to explore a new city are free walking tours in which a local guide takes you around the city, shows you interesting places and shares their knowledge with you. At the end of these tours, guides often share their favorite spots and \u0026lsquo;secret tips\u0026rsquo; with you, usually as a way to keep contact or get you to follow their Instagram. We\u0026rsquo;d take these recommendations and, if there is time, plan some of our travel days around visiting them, eating there or whatever else makes sense.","title":"Wanderlist - Planning"},{"content":"The problem Interviews are great. You can learn a lot about a topic by talking to experts. If you are working on a product, talking to users is common advice. If you are a researcher, qualitative surveys using interviews are great fun.\nTranscribing interviews, however, sucks\nDoing it yourself is tedious and is a painfully acquired skill. Outsourcing work to one of the various transcription APIs also is no perfect solution. Either you feed the beast that is large-scale AI models or pay even more money to trust Google, Amazon and co to not use your data.\nAI-powered local transcription Whisper, recently released as open-source software by OpenAI, sounds like an alternative. It promises to be able to transcribe multiple languages, locally on your own workstation. Is the quality usable? It turns out, with a bit of setup, the answer is yes.\nI conducted some interviews using the following process and transcribed them locally using Whisper. From start to finish, the transcription takes me roughly twice the interview time (so 120 mins for a 60 mins interview), half of which is my poor laptop struggling to auto-transcribe the interview.\nAs input, the setup uses separate audio files for the interviewer and participant. These can be created using Zoom and selecting \u0026ldquo;Record a separate audio file for each participant\u0026rdquo; in Settings, Recording.\nTo decrease random noise, microphones should be muted when not speaking. In video calls, nodding is best to encourage participants to continue to talk - you\u0026rsquo;ll need to delete less \u0026ldquo;Yeah\u0026quot;s this way.\nThe Code All code for the setup is released in the GitHub repository rhazn/whisper-local-transcribe. A description of the process and working notebook can be found there as transcribe.ipynb.\nAfter automatically transcribing the interview, some manual cleanup is still needed. Helpful tools for this are VLC to listen to the audio with Global Hotkeys configured to pause/play and skip ahead/back 10s.\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/whisper-ai-local-transcribe/","summary":"The problem Interviews are great. You can learn a lot about a topic by talking to experts. If you are working on a product, talking to users is common advice. If you are a researcher, qualitative surveys using interviews are great fun.\nTranscribing interviews, however, sucks\nDoing it yourself is tedious and is a painfully acquired skill. Outsourcing work to one of the various transcription APIs also is no perfect solution.","title":"AI-powered local interview transcription with Whisper"},{"content":"I ask students that want to write a thesis with me to prepare an exposé (or project outline) before we start. For the student, this ensures they have an idea of the work involved and a reasonable timeline to complete it. For me, it provides a short overview about what we agreed on and highlights important deadlines.\nAt a minimum, I think an exposé should be a roughly two-page PDF and include:\nA clear goal A structure (type, approach, table of contents) Work packages A timeline Importantly, the exposé is not graded and it is not used for evaluation. During research, it is normal that timelines change and work packages might need to be adjusted. The contents of the exposé are just meant as estimations to have a similar vision before starting a project.\nAn easy way to generate this PDF is using Latex, for example with overleaf.com. I\u0026rsquo;ve included some starter code and an example exposé here. If you want to see a real life example, my exposé for my master thesis can be downloaded, too.\nWhen I write about thesis type or structure, I am referring to them as used on our thesis portal.\nExample code \\documentclass{article} \\usepackage[english]{babel} \\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry} \\usepackage{amsmath} \\usepackage[colorlinks=true, allcolors=blue]{hyperref} \\title{Provisional Title of Your Thesis} \\author{You \\\u0026amp; Your Advisor} \\begin{document} \\maketitle \\section{Introduction} The introduction describes your topic and why it is important. You can also include information about related work in case you already know about it or want to work ahead. The introduction section should end in a clearly formulated goal for your thesis. For a research-based thesis this will be a research question, for an engineering thesis an engineering challenge. \\section{Structure} In a structure section, you include what type of thesis you are writing and what scientific methods you plan to use. This is also the place to propose a table of contents that makes sense to you. In the case of a literature review, a method reference could be Kitchenham \\cite{Kitchenham2004-wb}. Or, if you are using design science, Peffers et al. \\cite{Peffers2007-yv}. A table of contents should be a simple list of headlines, e.g, for a design science thesis: \\begin{enumerate} \\item Introduction \\item Problem identification \\item Objective definition \\item Solution design \\item Implementation \\item Demonstration \\item Evaluation \\item Conclusions \\end{enumerate} \\section{Expected Work Packages} Planning out work packages gives you a chance to break down your work into smaller tasks. Work packages do not have to follow a strict structure, but it is helpful if you assign a clear identifier to each. Your work packages are a plan for you and are not part of an evaluation of your work, so feel free to include as much detail as you want. \\begin{itemize} \\item \\textbf{WP1:} Write an introduction \\begin{enumerate} \\item \\textbf{WP1.1:} Search for related work \\end{enumerate} \\item \\textbf{WP2:} Write a structure \\item \\textbf{WP3:} Write expected work packages \\item \\textbf{WP4:} Write an expected timeline \\end{itemize} \\section{Expected Timeline} A timeline is important to be on the same page with your advisor and must include an idea of when you want to start your work, when it should be finished and mention any deadlines you have. You should plan some milestones for yourself to check your progress periodically. This section, like the work packages, is for you and does not influence your grading. For bonus points, include a Gantt chart referencing the work packages you previously defined. Consider using a text-based tool to generate your timeline diagram, for example MermaidJS\\footnote{https://mermaid.js.org/syntax/gantt.html} for Markdown or pgfgantt\\footnote{https://www.ctan.org/pkg/pgfgantt} for LateX. \\begin{filecontents}{bibliography.bib} @ARTICLE{Peffers2007-yv, title = \u0026#34;A Design Science Research Methodology for Information Systems Research\u0026#34;, author = \u0026#34;Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A and Chatterjee, Samir\u0026#34;, journal = \u0026#34;Journal of Management Information Systems\u0026#34;, publisher = \u0026#34;Routledge\u0026#34;, volume = 24, number = 3, pages = \u0026#34;45--77\u0026#34;, month = dec, year = 2007, issn = \u0026#34;0742-1222\u0026#34;, doi = \u0026#34;10.2753/MIS0742-1222240302\u0026#34; } @ARTICLE{Kitchenham2004-wb, title = \u0026#34;Procedures for performing systematic reviews\u0026#34;, author = \u0026#34;Kitchenham, Barbara\u0026#34;, journal = \u0026#34;Keele, UK, Keele University\u0026#34;, publisher = \u0026#34;elizabete.com.br\u0026#34;, volume = 33, number = 2004, pages = \u0026#34;1--26\u0026#34;, year = 2004 } \\end{filecontents} \\bibliographystyle{plain} \\bibliography{bibliography} \\end{document} Downloads Example Exposé\nMy Exposé\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/thesis-expose-project-outline/","summary":"I ask students that want to write a thesis with me to prepare an exposé (or project outline) before we start. For the student, this ensures they have an idea of the work involved and a reasonable timeline to complete it. For me, it provides a short overview about what we agreed on and highlights important deadlines.\nAt a minimum, I think an exposé should be a roughly two-page PDF and include:","title":"Write an exposé for your thesis"},{"content":"Our paper \u0026ldquo;Challenges to Open Collaborative Data Engineering\u0026rdquo; got accepted to the Hawaii International Conference on System Sciences 2023 (HICSS 56). Sadly, we could not present in person due to a flight cancellation but we provided a short video presentation.\nAbstract Open data is data that can be used, modified, and passed on, for free, similar to open-source software. Unlike open-source, however, there is little collaboration in open data engineering. We perform a systematic literature review of collaboration systems in open data, specifically for data engineering by users, taking place after data has been made available as open data. The results show that open data users perform a wide range of activities to acquire, understand, process and maintain data for their projects without established best practices or standardized tools for open collaboration. We identify and discuss technical, community, and process challenges to collaboration in data engineering for open data.\nLinks Paper on ScholarSpace\nHICSS-56 proceedings\nDownloads Paper local\nSlides presentation\nVideo presentation\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/challenges-to-open-collaborative-data-engineering/","summary":"Our paper \u0026ldquo;Challenges to Open Collaborative Data Engineering\u0026rdquo; got accepted to the Hawaii International Conference on System Sciences 2023 (HICSS 56). Sadly, we could not present in person due to a flight cancellation but we provided a short video presentation.\nAbstract Open data is data that can be used, modified, and passed on, for free, similar to open-source software. Unlike open-source, however, there is little collaboration in open data engineering. We perform a systematic literature review of collaboration systems in open data, specifically for data engineering by users, taking place after data has been made available as open data.","title":"Challenges to Open Collaborative Data Engineering"},{"content":"At the start of July, I visited the Softwarecampus Kickoff Event. The Softwarecampus is a project targeting self-described \u0026ldquo;EntrepreNerds\u0026rdquo; and sponsoring research projects with an industry partner with up to 100k€. Having been to a few ice-breaker events with fellow nerds I anticipated an awkward day but I was pleasantly surprised. In the hope of being pleasantly surprised again in the future, I wanted to share some ways the Softwarecampus team made the event stand out to me.\nThe environment: The kickoff event took place at the Forum Digitale Technologien in Berlin. The room we used was a large, open space and included a small exhibition of other research projects. Together with the relaxed pacing of the event, this provided interesting things to talk about and felt less like a forced getting to know people and more like an exhibition tour.\nShort, random group chats: This was the more classically-awkward part of the event but it was still well done. We were assigned to small (3-5 person) groups to present and talk about our projects to the others in about five minutes. The groups were created by gathering around home towns first and larger research areas later.\nCollaborative comic writing: This was the highlight of the event for me. With our last group (organized by research area so everyone roughly knows what the other people are doing), we were asked to choose a hero and create a three-panel comic/story that included references to our projects and how they are used to overcome challenges. We chose Alice in Wonderland and built a beautifully absurd story about how Alice has to find good open data (my project) to convince the sexist Mad Hatter and his bunnies that they are wrong and she should be let into Wonderland. While we talked about our story, artists walked from group to group and drew the comic panels from descriptions (edited to add a link to their business: Miss Vizzz). Because we had to try to find a way to fit references to all our projects into the story, we had to discuss what we were doing in detail but always with the absurd background of helping Alice into Wonderland. Finally, we had to explain our story (and by extension our projects) to the artists and present our story to the group. All of this made for a fun activity that, without feeling forced, communicated a lot of info about everyone\u0026rsquo;s goals.\nIn the end, we voted on who had the best story and presentation, we did not win (which, for the record I heavily disagree with and is a scandal) and went on to have a nice dinner. So, thank you Softwarecampus-Team (Susanne \u0026amp; Stefan) and other people planning kickoff events for nerds: Consider hiring artists and drawing some comics.\nKickoff Participants\n","permalink":"https://heltweg.org/posts/swc-kickoff/","summary":"At the start of July, I visited the Softwarecampus Kickoff Event. The Softwarecampus is a project targeting self-described \u0026ldquo;EntrepreNerds\u0026rdquo; and sponsoring research projects with an industry partner with up to 100k€. Having been to a few ice-breaker events with fellow nerds I anticipated an awkward day but I was pleasantly surprised. In the hope of being pleasantly surprised again in the future, I wanted to share some ways the Softwarecampus team made the event stand out to me.","title":"Softwarecampus 2022 - How to run a kickoff event for nerds"},{"content":" \u0026ldquo;There are only two hard things in Computer Science: cache invalidation and naming things.\u0026rdquo;\n- Phil Karlton\nAngularJS was released in 2010 and revolutionized modern frontend development. In 2016, the AngularJS team published a new framework, written in TypeScript and incompatible with AngularJS. They also made the baffling choice to call this framework Angular 2.\nMayhem ensued, confused developers talked to each other about incompatible frameworks. The excellent community documentation AngularJS had built in the form of blogs and StackOverflow answers became an active hindrance for people trying to learn Angular. Undeterred by this, the Angular team doubled down and kept releasing incompatible framework versions, only distinguished by a number.\nFive years later, the team announced that AngularJS would be discontinued. Immediately they needed to clarify that they would not discontinue Angular but only AngularJS.\nAngular clearing up confusion, five years later\nEven with the best documentation, users are going to google for information. They are going to read blogs, ask on StackOverflow and copy answers. So to make it easy on them, please:\nUse unique names for major versions (\u0026ldquo;Android KitKat\u0026rdquo; instead of \u0026ldquo;Android 4.4\u0026rdquo;). They are easier to include in searches. Use error codes (like \u0026ldquo;Error 34\u0026rdquo;) or slugs (like \u0026ldquo;ERROR_SOME_NAME\u0026rdquo;) in addition to localized, readable messages. Unique strings enable users with different languages than English to contribute. As a user, use Software in English. It makes it easier to google for errors ;). About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/chose-names-for-google-not-people/","summary":"\u0026ldquo;There are only two hard things in Computer Science: cache invalidation and naming things.\u0026rdquo;\n- Phil Karlton\nAngularJS was released in 2010 and revolutionized modern frontend development. In 2016, the AngularJS team published a new framework, written in TypeScript and incompatible with AngularJS. They also made the baffling choice to call this framework Angular 2.\nMayhem ensued, confused developers talked to each other about incompatible frameworks. The excellent community documentation AngularJS had built in the form of blogs and StackOverflow answers became an active hindrance for people trying to learn Angular.","title":"Choose names for Google, not people"},{"content":"It is a beautiful rite of passage for a bright-eyed junior developer to join a team, take some tasks full of enthusiasm, and have the life and joy sucked out of them one sprint at a time. Soon enough, they sit in planning meetings, miserably complaining, accusingly asking who wrote that shit. Their transformation to a full team member is complete. They have become one of us.\nI was once that bright-eyed junior developer. Many encounters with PHP4 later could, I no longer look forward to creating new features. Instead, I looked back and complained about old ones. Trashing legacy software is fun. It creates camaraderie. Who wrote this? What were they thinking? Looking at the commit log showed names that meant nothing to me. Ghosts that moved on long ago, leaving nothing but their terrible code.\nOne day, I was pair programming with my friend Torben. Torben was a senior developer who had introduced me to the team months earlier. When I found some weird code, I followed the tradition. Who was dumb enough to write this shit? I opened the commit log, and for the first time, I recognized a name: \u0026ldquo;Torben.\u0026rdquo;\nSince then, I remind myself that legacy software is written by people like me. Torben had constraints I knew nothing about. He worried about deadlines long past. Torben had learned a lot since then. Development workflows had evolved, infrastructure had gotten better.\nAnd to all the junior developers who have complained to my face about my trash code: I forgive you, I understand.\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/who-wrote-this-shit/","summary":"It is a beautiful rite of passage for a bright-eyed junior developer to join a team, take some tasks full of enthusiasm, and have the life and joy sucked out of them one sprint at a time. Soon enough, they sit in planning meetings, miserably complaining, accusingly asking who wrote that shit. Their transformation to a full team member is complete. They have become one of us.\nI was once that bright-eyed junior developer.","title":"Who wrote this shit?"},{"content":"Abstract Belief change research investigates how agents adapt their knowledge with potentially conflicting information. A common formalization is by epistemic states, abstract entities often represented by faithful preorders. Operators describe how epistemic states change with new evidence and are classified by which postulates they satisfy. Different approaches have been suggested for the problem of iterated belief change. Recent work introduces uniform revision that revises an agent\u0026rsquo;s beliefs based on one static total preorder, therefore lowering representational costs.\nIn this thesis, an extended epistemic state approach is introduced, based on an agent deterministically switching between total preorders. Challenges for implementations in the area of iterated belief change, like textual representation of total preorders, are pointed out and solutions developed. A tool for the automated certification of postulates for iterated belief change, called Coeus, is implemented for the new operator. Finally, the developed software is evaluated empirically. Coeus is publicly available, and most of its code is open-source.\nDownload Master thesis local\nMaster thesis colloquium slides\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/master-thesis/","summary":"Abstract Belief change research investigates how agents adapt their knowledge with potentially conflicting information. A common formalization is by epistemic states, abstract entities often represented by faithful preorders. Operators describe how epistemic states change with new evidence and are classified by which postulates they satisfy. Different approaches have been suggested for the problem of iterated belief change. Recent work introduces uniform revision that revises an agent\u0026rsquo;s beliefs based on one static total preorder, therefore lowering representational costs.","title":"Master Thesis: Implementing a Structured Approach to Belief Revision by Deterministic Switching Between Total Preorders"},{"content":"Note: An extended version of this paper has been published at the 7th Workshop on Formal and Cognitive Reasoning, you can find it here.\nOur paper \u0026ldquo;Certification of Iterated Belief Changes via Model Checking and its Implementation\u0026rdquo; got accepted to the 19th International Workshop on Non-Monotonic Reasoning (NMR-2021) at the 18th International Conference on Principles of Knowledge Representation and Reasoning (KR 2021). During the workshop I held a short presentation of the results. You can find both the paper as well as the presentation slides here.\nAbstract Iterated belief change investigates principles for changes on epistemic states and their representational groundings. A common realisation of epistemic states are total preorders over possible worlds. In this paper, we consider the problem of certifying whether an operator over total preorders satisfies a given postulate. We introduce the first-order fragment FOTPC for expressing belief change postulates and present a way to encode information on changes into an FOTPC-structure. As a result, the question of whether a belief change fulfils a postulate becomes a model checking problem. We present Alchourron, an implementation of our approach, consisting of an extensive Java library, and also of a web interface, which suits didactic purposes and experimental studies.\nDownload NMR-2021 Paper local\nNMR-2021 Workshop Proceedings\nNMR-2021 Presentation Slides\nWorkshop Presentation Video Video Link\nVideo Local\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/certification-of-iterated-belief-changes-via-model-checking-and-its-implementation/","summary":"Note: An extended version of this paper has been published at the 7th Workshop on Formal and Cognitive Reasoning, you can find it here.\nOur paper \u0026ldquo;Certification of Iterated Belief Changes via Model Checking and its Implementation\u0026rdquo; got accepted to the 19th International Workshop on Non-Monotonic Reasoning (NMR-2021) at the 18th International Conference on Principles of Knowledge Representation and Reasoning (KR 2021). During the workshop I held a short presentation of the results.","title":"Certification of Iterated Belief Changes via Model Checking and its Implementation"},{"content":"Our paper \u0026ldquo;On Using Model Checking for the Certification of Iterated Belief Changes\u0026rdquo; got accepted to the 7th Workshop on Formal and Cognitive Reasoning (FCR-2021) at the 44th German Conference on Artificial Intelligence (KI-2021). During the workshop I held a short presentation of the results. You can find both the paper as well as the presentation slides here.\nAbstract The theory of iterated belief change investigates how epistemic states are changed according to new beliefs. This is typically done by focussing on postulates that govern how epistemic states are changed. A common realisation of epistemic states are total preorders over possible worlds. In this paper, we consider the problem of certifying whether an operator over total preorders satisfies a given postulate. We introduce the first-order fragment FOTPC for expressing belief change postulates and present a way to encode information on changes into an FOTPC-structure. As a result, the question of whether a belief change fulfils a postulate becomes a model checking problem. We developed Alchourron, an implementation of our approach, consisting of an extensive Java library, and also of a web interface, which suits didactic purposes and experimental studies. For Alchourron, we also present an evaluation of the running time with respect to logical properties.\nDownload FCR-2021 Paper local\nFCR-2021 Paper from CEUR Workshop Proceedings\nFCR-2021 Presentation Slides\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/on-using-model-checking-for-the-certification-of-iterated-belief-changes/","summary":"Our paper \u0026ldquo;On Using Model Checking for the Certification of Iterated Belief Changes\u0026rdquo; got accepted to the 7th Workshop on Formal and Cognitive Reasoning (FCR-2021) at the 44th German Conference on Artificial Intelligence (KI-2021). During the workshop I held a short presentation of the results. You can find both the paper as well as the presentation slides here.\nAbstract The theory of iterated belief change investigates how epistemic states are changed according to new beliefs.","title":"On Using Model Checking for the Certification of Iterated Belief Changes"},{"content":"I created this synopsis of some ideas from the paper \u0026ldquo;How to Revise a Total Preorder\u0026rdquo; by Booth and Meyer during the seminar on \u0026ldquo;Representation and processing of uncertain knowledge with logic-based methods\u0026rdquo; at the University of Hagen.\nThe slides are from my final presentation of the topic (and might be more or less useless on their own).\nAbstract Adapting one’s world view in the light of new information is a central skill of intelligent agents. Total preorders are a common tool to model plausibility orderings over possible worlds in the research field of belief change. In their paper ”How to Revise a Total Preorder”, Booth and Meyer present an approach to revising preorders for iterated belief revision. Their operator is based on assigning abstract intervals of plausibility to worlds, depending on new evidence supporting them or not. This synopsis presents part of their work in tpo-revision operators and their properties with the help of an accompanying example and additional visualisation.\nDownload Seminar Paper\nPresentation Slides\nSource Code\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/judges-aliens-and-total-preorders/","summary":"I created this synopsis of some ideas from the paper \u0026ldquo;How to Revise a Total Preorder\u0026rdquo; by Booth and Meyer during the seminar on \u0026ldquo;Representation and processing of uncertain knowledge with logic-based methods\u0026rdquo; at the University of Hagen.\nThe slides are from my final presentation of the topic (and might be more or less useless on their own).\nAbstract Adapting one’s world view in the light of new information is a central skill of intelligent agents.","title":"Of judges, aliens and total preorders"},{"content":"\u0026ldquo;You\u0026rsquo;ll never forget your first dragon\u0026rdquo; used to be the promise of Tibia, a free 2D-MMO I played as a young teenager. I spend day after day playing Tibia, training skills with friends and exploring the world and still\u0026hellip; never got to kill a dragon.\nComing back With nostalgia rekindled by quickly burning out on WoW: Shadowlands I tried to check in my old account. Sadly, I did not only forget the password but also lost control of my email address. Tibia allows you to restore access by sending an actual letter to the registered address of the account though. I entered my parents address, waited two weeks and actually got access to my account back.\nSadly it seems to be frozen due to hackers using it :(. I had to write support to actually unblock it.\nA glimmer of hope\nWhile waiting for my account to be unblocked I went digging\u0026hellip; The account said it was created in 2010, that is definitely too late, I remember playing as a kid/teenager and now I am old.\nBut I found some references still to my old Tibia life (Ranarion used to be my character name):\n\u0026ldquo;I heard you die twice. Once when they bury you in the grave. And the second is the last time when someone mentions your name.\u0026rdquo; - Irvin D. Yalom\nI used to play with a guild called Valheru on Antica. I never was a member because the minimum level required was 50 and I never made it above 18. But they took me under their wings, let me train with them and took me on some raids. It was a nice time filled with late nights trying to make my sorcerer better at shielding and 120€ internet bills.\nA lot of emails and security questions later my account was officially back and I could play again. As with all modern things Tibia now has a premium account (which I bought to upgrade my character from a Sorcerer to a Master Sorcerer).\nMaster Sorcerer Rhazn\nAfter I satisfied my nostalgia I was content to just let my premium time run out and not come back. But then I thought - could I actually find a fitting end for my 10+ year Tibia journey? What about the promise CipSoft made to me?\nI asked around how and where a level 35 Master Sorcerer might be able to solo kill a dragon and found out that some of them spawn near the old dwarven city of Kazordoon. Apparently it is also very helpful to summon demon skeletons as blockers because they are immune to fire.\nArmed with new knowledge, 120 mana potions and my two trusty companion skeletons I took a deep breath and entered \u0026ldquo;Kazordoon Dragon Lair\u0026rdquo;. When I saw the dragon on my radar I targeted it and ran away while spamming \u0026ldquo;Ice Strike\u0026rdquo;. 10+ years brought to a head in one moment:\nThe beast is dead\nThe actual fight was easy and fast, my skeleton friends and ice magic slaughtered the poor dragon quickly and \u0026ldquo;Mana Shield\u0026rdquo; allowed my Sorcerer to never be in any danger.\nI ran back to the main city Thais to log out somewhere but figured\u0026hellip; why not see if the old place me and the Valheru guild used to train and life still is around?\nThe guildhall an officer called \u0026ldquo;Kitiara\u0026rdquo; rented was called \u0026ldquo;Halls of Adventurers\u0026rdquo;. Meeting Kitiara and advertising her tavern with a \u0026ldquo;global say\u0026rdquo; spell was what introduced me to Valheru originally so I figured it would be a fitting end.\nWhen I reached the hall and inspected it, I saw that not only did it still exist, Kitiara was still renting it. When I tried to enter my character got immediately teleported out of course - my character was long forgotten.\nI logged out one last time in front of the halls, a fitting end for my Tibia story - you never forget your first dragon.\nSleep now\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/how-to-kill-a-dragon/","summary":"\u0026ldquo;You\u0026rsquo;ll never forget your first dragon\u0026rdquo; used to be the promise of Tibia, a free 2D-MMO I played as a young teenager. I spend day after day playing Tibia, training skills with friends and exploring the world and still\u0026hellip; never got to kill a dragon.\nComing back With nostalgia rekindled by quickly burning out on WoW: Shadowlands I tried to check in my old account. Sadly, I did not only forget the password but also lost control of my email address.","title":"How to kill a dragon"},{"content":"A social media analytics tool, powered by a Java/PostgreSQL/Kubernetes backend and a React frontend.\nLandingpage Case Study Index New Report Report Example Report Example Report Example Comments by Word Comments by Sentiment ","permalink":"https://heltweg.org/posts/spaeher/","summary":"A social media analytics tool, powered by a Java/PostgreSQL/Kubernetes backend and a React frontend.\nLandingpage Case Study Index New Report Report Example Report Example Report Example Comments by Word Comments by Sentiment ","title":"spaeher.app - sentiment analytics for youtube"},{"content":"Finding needles There has never been as much information easily accessible to anyone as right now. The ease of publishing your own writing leads to new problems: The question is no longer where do you find content about a topic but what content is good and worth your time?\nTo help you find the needle in the biggest haystack ever, here is my personal list of good content that influenced me.\nThe list \u0026ldquo;Things You Should Never Do\u0026rdquo; by Joel Spolsky One of the main things I have learned when going from a young software engineer to working professionally for years is the value of iteration. I\u0026rsquo;ve personally been burned by completely rewriting projects and ever since keep this article around to remind myself of that. Also of the pain of living through the javascript framework frontend wars.\nThings You Should Never Do, Part 1\n\u0026ldquo;Choose Boring Technology\u0026rdquo; by Dan McKinley Years ago I read a blog post titled \u0026ldquo;Why I write Java\u0026rdquo; that basically boiled down to the author writing Java and using established technologies to focus on the product and not the tech. The blog\u0026rsquo;s first comment was \u0026ldquo;So you write java because you are an old, boring guy\u0026rdquo;. Sadly, that gem of internet discussion seems to be lost to time. This article presents the same ideas of cautioning against new technology (but not being dogmatic about never using it) and as a fellow old, boring person now I can see myself in it.\nChoose Boring Technology\n\u0026ldquo;The Product-Minded Software Engineer\u0026rdquo; by Gergely Orosz Job identities are hard. Am I a software engineer that wants to build an amazing algorithm? Am I a \u0026ldquo;creator\u0026rdquo; that wants to build a product? Am I a manager that wants to build a team? If you struggle to pin down what you actually want to do, this article introduces yet another category to fit your life into.\nThe Product-Minded Software Engineer\n\u0026ldquo;Give it five minutes\u0026rdquo; by Jason Fried Jason Fried and anything Basecamp are an interesting source for thoughtful content on business, software engineering and as it turns out even life advice ;). This is a really short blogpost that reminds me to keep an open mind and be humble.\nGive it five minutes\n\u0026ldquo;Are Passions Serendipitously Discovered or Painstakingly Constructed?\u0026rdquo; by Cal Newport A short blog for people being lost in life. If you are unsure about your goal in life or why you have no passions, maybe this is something to give you ideas. Also it has a headline that literally is \u0026ldquo;Short Case Study #2: The Bored Programmer\u0026rdquo;. If that does not fit the audience of this article, nothing will.\nAre Passions Serendipitously Discovered or Painstakingly Constructed?\n\u0026ldquo;Function + Feeling\u0026rdquo; by Haraldur Thorleifsson This is a talk and not a blog but it fits the list. In a world focused on monetizing and optimizing everything, talking about predatory business models and the death of privacy this was a nice reminder that our work can have real, positive impact on people and it\u0026rsquo;s worth fighting for that.\nFunction + Feeling\n\u0026ldquo;Salary Negotiation: Make More Money, Be More Valued\u0026rdquo; and \u0026ldquo;Don\u0026rsquo;t Call Yourself A Programmer, And Other Career Advice\u0026rdquo; by Patrick McKenzie For software engineers just starting a career I think these blogs are an absolute must read to navigate the business side (not only of coding but for anything). Since reading these my opinions on the approach to working itself has changed quite a bit but I still consider them a solid into.\nSalary Negotiation: Make More Money, Be More Valued\nDon\u0026rsquo;t Call Yourself A Programmer, And Other Career Advice\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/wisdom-from-the-internet/","summary":"Finding needles There has never been as much information easily accessible to anyone as right now. The ease of publishing your own writing leads to new problems: The question is no longer where do you find content about a topic but what content is good and worth your time?\nTo help you find the needle in the biggest haystack ever, here is my personal list of good content that influenced me.","title":"Wisdom from the Internet, content that influenced how I think about software (and life)."},{"content":"Screenshots Login Intro slides Intro slides Setting up the user language Setting up the user language Profile view Write a letter List of your letters Letter feed See your own letter Comment on other letters ","permalink":"https://heltweg.org/posts/iyagi/","summary":"Screenshots Login Intro slides Intro slides Setting up the user language Setting up the user language Profile view Write a letter List of your letters Letter feed See your own letter Comment on other letters ","title":"IYAGI - postcards from friends"},{"content":"The popularity of machine learning, data science and related disciplines is exploding and with it the amount of courses, books, block posts etc you are exposed to. I recently finished the relatively old but highly rated course Machine Learning by Stanford University on Coursera and wanted to take the chance to offer my review and notes I took.\nThe course Although the course is old enough to be referred to as \u0026ldquo;classic\u0026rdquo; by quite a few descriptions I have read it is timeless in the sense that most good introductions are. It covers concepts from tools in ML like supervised- or unsupervised learning, tips for their application with algorithm rating and debugging and much more. Most topics are covered very timeless with the theory and math behind them being the focus.\nCoursera offers video lectures, lecture notes (as pdf) and a short text summary for most of the 11 weeks of the course. Grading is done using short quizzes at the end of each topic as well as programming exercises in Octave or Mathlab. Forums are available to help with any questions and mentors are good at answering anything that comes up.\nThe good The video lectures are really well done and the content seems excellent and very \u0026ldquo;grounded\u0026rdquo;. It is definitely not a course trying to cash in on the hype surrounding ML.\nThe programming exercises are solid and introduced me well to a different side of software development that is more focussed on solving complex mathematical problems than displaying a centered div.\nThe forums and additional resources prepared by the mentors are very good, in every case where I had a problem it was already solved by someone else asking it in the forums.\nIf you don\u0026rsquo;t want to get a certificate the course is completely free.\nThe bad There is a noticeable drop in quality in the later weeks. While the content stays excellent the written summaries disappear, sometimes obvious double takes that should have been edited are left in videos and programming exercises have a lot of pre-written code in their descriptions.\nSolving programming assignments in Octave: Probably due to the age of the course the setup of Octave is not friction free - in the version I used the \u0026ldquo;pause\u0026rdquo; function was broken which made all programming assignments hang at the first stop. It was relatively easy for me to debug and fix by overwriting the internal pause function but I\u0026rsquo;ve seen quite a few questions about Octave issues in the forum. In addition Octave seems to not be the primary choice in the ML community but since the course teaches concepts over concrete implementation that is fine by me.\nNotes I took a lot of notes during the course (more than 120 pages to be exact). They are mostly very close to the course material provided but sometimes rephrased and annotated with additional comments from me. In case they help anyone you can: Download them here\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/machine-learning-basics-machine-learning-by-stanford-university-review-and-notes/","summary":"The popularity of machine learning, data science and related disciplines is exploding and with it the amount of courses, books, block posts etc you are exposed to. I recently finished the relatively old but highly rated course Machine Learning by Stanford University on Coursera and wanted to take the chance to offer my review and notes I took.\nThe course Although the course is old enough to be referred to as \u0026ldquo;classic\u0026rdquo; by quite a few descriptions I have read it is timeless in the sense that most good introductions are.","title":"Machine learning basics: Machine Learning by Stanford University (Coursera) review and notes"},{"content":"A tweet in the database is worth two in the API Working with tweets from the twitter API probably means importing data into your own database - the standard API does not provide historical data (only the last seven days) and has various rate limits.\nSo regardless of the final goal in this blog we\u0026rsquo;ll explore importing tweets from the API into a database for future use. All done with NodeJS, written in Typescript and utilizing MongoDB as data store.\nBig numbers, big problems Once you authenticate with the API and pull in the first tweets (for example using the twitter module on npm) you will notice tweets contain ids as numbers and \u0026ldquo;id_str\u0026rdquo; which is the same id just as string:\n{ \u0026#34;created_at\u0026#34;: \u0026#34;Wed Oct 10 20:19:24 +0000 2018\u0026#34;, \u0026#34;id\u0026#34;: 1050118621198921728, \u0026#34;id_str\u0026#34;: \u0026#34;1050118621198921728\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;To make room for more expression, we will now count all emojis as equal—including those with gender‍‍‍ ‍‍and skin t… https://t.co/MkGjXf9aXm\u0026#34;, \u0026#34;user\u0026#34;: {}, \u0026#34;entities\u0026#34;: {} } The reason for this is that some languages (Javascript being one of them) can not work with big numbers. For example JS numbers are internally 64-bit floats and only use the first 53 bits for the integer value. Javascript provides the static property Number.MAX_SAFE_INTEGER as 9007199254740991 which is smaller than the id in the example tweet already.\nTo work with tweet ids we need a way to handle bigger numbers and use the \u0026ldquo;id_str\u0026rdquo;. big.js provides that functionality and is used in all following code examples.\nSaving tweets Saving tweets in MongoDB is easy. Since we are using typescript we can rely on the excellent (Typegoose library)[https://github.com/typegoose/typegoose] to create models for tweets and interact with MongoDB:\nimport { prop, Typegoose, index } from \u0026#34;@hasezoey/typegoose\u0026#34;; @index({ \u0026#34;entities.user_mentions.screen_name\u0026#34;: 1 }) export class TwitterStatus extends Typegoose { @prop({ required: true, unique: true, index: true }) id_str!: string; @prop({ required: true }) full_text!: string; @prop({ required: true }) entities!: { user_mentions: { screen_name: string }[] } @prop({ required: true }) created_at!: string; } export const TwitterStatusModel = new TwitterStatus().getModelForClass(TwitterStatus, { schemaOptions: { strict: false } }); Notice I only defined some properties I wanted to use in this model \u0026amp; the index is also related to my use case. You might need to change those depending on the project.\nIf schemaOptions define strict as false (see the last line) typegoose saves the whole JSON of the tweet in MongoDB, not just defined fields.\nImport logic To optimize the amount of tweets you can crawl from the API in the limits twitter provides an excellent resource on using the since_id and max_id parameters correctly here: https://developer.twitter.com/en/docs/tweets/timelines/guides/working-with-timelines.\nIn summary this means:\nset the since_id to the highest tweet id your application has already imported defining a lower bound for the imported tweets set the max_id to the max_id from the last import and subtract 1 defining the upper bound import tweets while setting max_id to the lowest id in the returned list until no new ones are returned, moving the upper bound closer to the lower bound once no new tweets are returned set max_id to undefined to remove the upper bound for future imports If you want to crawl all mentions for an account you can keep track of your crawl status with this model:\nimport { prop, Typegoose } from \u0026#34;@hasezoey/typegoose\u0026#34;; export class TwitterCrawlStatus extends Typegoose { @prop({ required: true, unique: true, lowercase: true, trim: true }) account!: string; @prop({ trim: true }) sinceId?: string; @prop({ trim: true }) maxId?: string; @prop({ trim: true }) overallMaxId?: string; } export const TwitterCrawlStatusModel = new TwitterCrawlStatus().getModelForClas(TwitterCrawlStatus); A basic algorithm without any safeguards against failing that uses that logic and imports all mentions for a specific account follows:\nwhile(true) { const twitterCrawlStatus = await TwitterCrawlStatusModel.findOne({ account: account }; if (!twitterCrawlStatus) { twitterCrawlStatus = await TwitterCrawlStatusModel.create({ account: account }); await twitterCrawlStatus.save(); } const tweets = await twitterService.getMentions( account, twitterCrawlStatus.sinceId ? Big(twitterCrawlStatus.sinceId) : undefined, twitterCrawlStatus.maxId ? Big(twitterCrawlStatus.maxId).minus(1) : undefined, ); if (tweets.length \u0026gt; 0) { await TwitterStatusModel.bulkWrite(tweets.map(tweet =\u0026gt; { return { updateOne: { filter: { id_str: tweet.id_str }, update: { $set: tweet }, upsert: true } } })); const lowestId = (getLowestId(tweets) as Big); const highestId = (getHighestId(tweets) as Big); twitterCrawlStatus.maxId = lowestId.toFixed(); if (!twitterCrawlStatus.overallMaxId || Big(twitterCrawlStatus.overallMaxId).lt(highestId)) { twitterCrawlStatus.overallMaxId = highestId.toFixed(); } } else { twitterCrawlStatus.sinceId = twitterCrawlStatus.overallMaxId; twitterCrawlStatus.maxId = undefined; } await twitterCrawlStatus.save(); if (tweets.length === 0) { break; } } The twitter service The twitter service itself is just a minimalist wrapper around the twitter npm module:\nimport * as Twitter from \u0026#34;twitter\u0026#34;; import { Status } from \u0026#34;twitter-d\u0026#34;; import Big from \u0026#34;big.js\u0026#34;; export class TwitterService { private client: Twitter; constructor( consumerKey: string, consumerSecret: string, bearerToken: string ) { this.client = new Twitter({ consumer_key: consumerKey, consumer_secret: consumerSecret, bearer_token: bearerToken }); } public async getMentions( account: string, sinceId?: Big | undefined, maxId?: Big | undefined ): Promise\u0026lt;Status[]\u0026gt; { return await this.client.get(\u0026#34;search/tweets\u0026#34;, { q: `@${account} -filter:retweets`, result_type: \u0026#34;recent\u0026#34;, count: 100, include_entities: true, tweet_mode: \u0026#34;extended\u0026#34;, since_id: sinceId ? sinceId.toFixed(0) : undefined, max_id: maxId ? maxId.toFixed(0) : undefined }).then(response =\u0026gt; { return response.statuses; }); } } About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/import-tweets-with-nodejs-and-the-twitter-api/","summary":"A tweet in the database is worth two in the API Working with tweets from the twitter API probably means importing data into your own database - the standard API does not provide historical data (only the last seven days) and has various rate limits.\nSo regardless of the final goal in this blog we\u0026rsquo;ll explore importing tweets from the API into a database for future use. All done with NodeJS, written in Typescript and utilizing MongoDB as data store.","title":"Analyzing twitter: Import tweets with NodeJS and the twitter API"},{"content":"As the final step in this long series of blogposts we are going to deploy a simple webapp in a docker container to my personal cloud. For context here is the personal cloud setup with Traefik/Let\u0026rsquo;s Encrypt (Run a personal cloud with Traefik, Let\u0026rsquo;s encrypt and Zookeeper).\nIn previous blogposts I also described how I built the app (Build a PWA in docker).\nApp deployment The deployment runs the docker container. If you follow my personal cloud setup make sure to use more than one replica as the nature of preemtive VMs means one replica might randomly get shut down.\nNote that the image will be set and updated by gitlab ci as described here: (Deploy to google kubernetes engine using gitlab ci).\nkind: Deployment apiVersion: extensions/v1beta1 metadata: name: ???-app spec: replicas: 3 template: metadata: labels: app: ???-app spec: terminationGracePeriodSeconds: 60 containers: - name: ???-app image: \u0026#34;eu.gcr.io/???/app:latest\u0026#34; The service and traefik ingress apiVersion: v1 kind: Service metadata: name: ???-app spec: selector: app: ???-app ports: - name: web port: 80 targetPort: 80 --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ???-app annotations: kubernetes.io/ingress.class: traefik traefik.frontend.passHostHeader: \u0026#34;false\u0026#34; traefik.frontend.priority: \u0026#34;1\u0026#34; spec: rules: - host: app.???.com http: paths: - path: / backend: serviceName: ???-app servicePort: web Updating the traefik config Updating the traefik config is important so traefik requests a new HTTPS certificate for the app from Let\u0026rsquo;s encrypt. You will need to add this line to the traefik toml file that is described here (Run a personal cloud with Traefik, Let\u0026rsquo;s encrypt and Zookeeper):\n[[acme.domains]] main = \u0026#34;app.???.com\u0026#34; DNS Now you can just point the A record of your domain to the traefik external IP and the rest will automatically be handled by your personal cloud :).\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/deploy-an-app-into-your-personal-cloud/","summary":"As the final step in this long series of blogposts we are going to deploy a simple webapp in a docker container to my personal cloud. For context here is the personal cloud setup with Traefik/Let\u0026rsquo;s Encrypt (Run a personal cloud with Traefik, Let\u0026rsquo;s encrypt and Zookeeper).\nIn previous blogposts I also described how I built the app (Build a PWA in docker).\nApp deployment The deployment runs the docker container.","title":"Deploy an app into your personal cloud"},{"content":"In previous blogposts I explained my concept of a personal cloud for my own projects (Kubernetes for Sideprojects) and how I set it up (Run a personal cloud with Traefik, Let\u0026rsquo;s encrypt and Zookeeper). I also showed how I packaged a PWA project with docker (Build a PWA in docker).\nWith all those ingredients ready to go the last hurdle to solve is building the docker image automatically as well as publishing it to a private container registry so I can deploy it to my cloud from there.\nOverview The goal of the setup is to:\nbuild a docker image from any git hash using gitlab ci push that docker image to a google container registry tagged with the git hash update the kubernetes service description in the personal cloud project to pull the new docker image Permissions To enable gitlab to do these actions on our behalf we need to set up service accounts. We need\none account that allows gitlab to upload a new docker image to the container registry an account that allows it to change our kubernetes setup in the personal cloud registry You can create these service accounts in the \u0026ldquo;IAM \u0026amp; admin\u0026rdquo; -\u0026gt; \u0026ldquo;Service accounts\u0026rdquo; section of google cloud. Make sure to download and save the generated json file.\nWe will also need to allow the personal cloud project to pull the docker image from the container registry that is in a different project. For that I followed this excellent blogpost by Alexey Timanovskiy (Using single Docker repository with multiple GKE projects).\nIn general the permissions needed to interact with docker images in the container registry are related to only \u0026ldquo;Storage\u0026rdquo; so for pulling docker images the account will need \u0026ldquo;Storage Object Viewer\u0026rdquo; permissions only. For adding images to the registry \u0026ldquo;Storage Admin\u0026rdquo;.\nPublish a docker image with gitlab ci To allow gitlab ci to use your service account you need to save the content of the json files as a base64 encoded variable in the backend. You can find the setting under \u0026ldquo;Settings\u0026rdquo; -\u0026gt; \u0026ldquo;CI /CD\u0026rdquo; -\u0026gt; \u0026ldquo;Variables\u0026rdquo;. Be careful with this data since it is security relevant. The variables here will be available as environment variables during your jobs.\nI use the following gitlab ci stage to build and publish a project. Note that it only runs manually and for master. In this case it uses the service account saved in GCLOUD_SERVICE_KEY:\npublish: stage: publish image: docker:19.03.1 services: - docker:dind variables: DOCKER_DRIVER: overlay script: - echo $GCLOUD_SERVICE_KEY | base64 -d \u0026gt; ${HOME}/gcloud-service-key.json - docker login -u \\_json_key --password-stdin https://eu.gcr.io \u0026lt; ${HOME}/gcloud-service-key.json - docker build -t eu.gcr.io/projectid/app:${CI_COMMIT_SHA} . - docker push \u0026#34;eu.gcr.io/projectid/app:\\${CI_COMMIT_SHA}\u0026#34; only: - master when: manual About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/build-a-docker-image-on-gitlab-ci-and-publish-it-to-google-container-registry/","summary":"In previous blogposts I explained my concept of a personal cloud for my own projects (Kubernetes for Sideprojects) and how I set it up (Run a personal cloud with Traefik, Let\u0026rsquo;s encrypt and Zookeeper). I also showed how I packaged a PWA project with docker (Build a PWA in docker).\nWith all those ingredients ready to go the last hurdle to solve is building the docker image automatically as well as publishing it to a private container registry so I can deploy it to my cloud from there.","title":"Build a docker image on gitlab ci and publish it to google container registry"},{"content":"In a previous blogpost I showed how I build and publish docker images on gitlab ci (Build a docker image on gitlab ci)\nMake sure to read that post first for an overview and permission setup.\nUpdate the kubernetes service with the new docker image You can easily set up a deploy step using google\u0026rsquo;s own cloud SDK docker images. Note the service account with the permissions to change the kubernetes setup is saved as \u0026ldquo;GCLOUD_K8S_KEY\u0026rdquo; variable here.\nThis job changes the image of my deployment for the app. You will need to change the last line in the script to whatever change you want to make to your kubernetes setup on deploy.\ndeploy: stage: deploy image: google/cloud-sdk:257.0.0 script: - echo $GCLOUD_K8S_KEY | base64 -d \u0026gt; ${HOME}/gcloud-k8s-key.json - gcloud auth activate-service-account --key-file ${HOME}/gcloud-k8s-key.json - gcloud config set project personal-cloud-project-id - gcloud config set compute/zone your-compute-zone - gcloud container clusters get-credentials production - kubectl set image deployment/???-app ???-app=eu.gcr.io/docker-project-id/app:${CI_COMMIT_SHA} only: - master when: manual About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/deploy-to-google-kubernetes-engine-using-gitlab-ci/","summary":"In a previous blogpost I showed how I build and publish docker images on gitlab ci (Build a docker image on gitlab ci)\nMake sure to read that post first for an overview and permission setup.\nUpdate the kubernetes service with the new docker image You can easily set up a deploy step using google\u0026rsquo;s own cloud SDK docker images. Note the service account with the permissions to change the kubernetes setup is saved as \u0026ldquo;GCLOUD_K8S_KEY\u0026rdquo; variable here.","title":"Deploy to google kubernetes engine using gitlab ci"},{"content":"With my personal cloud setup based on kubernetes done (you can read about it here: https://heltweg.org/posts/run-a-personal-cloud-with-traefik-lets-encrypt-and-zookeeper/) it is time to actually deploy the first project into it.\nThe easiest application to deploy is a pure client side single page application, packaged in a docker container with a webserver like nginx to deliver the files. Packaging the application into it\u0026rsquo;s own container allows us to build a standardized container that can be run locally for testing or deployed to docker swarm and kubernetes.\nSetting up and configuring our own HTTP server also allows for fine tuning of caching to achieve good lighthouse scores:\nBuilding in docker For this setup we build the app using docker. That way the app is always built with the same node version and can be consistently reproduced, regardless of installed software on the local computer.\nThe project here is a react application based on create-react-app but it works similarly with any frontend framework:\nFROM node:12.6.0 AS build WORKDIR / COPY package.json package-lock.json tsconfig.json ./ RUN npm ci COPY ./src ./src COPY ./public ./public RUN npm run build --prod Configuring nginx For the nginx config I placed a config file into the project and checked it in. This config file is later on copied into the container that serves the SPA. To achieve good performance we\nenable gzip for HTML/CSS and JS files set up caching for any file for one year (because create-react-app builds new file names with each production build that invalidates the cache on deploy) disable the cache for the actual index.html file (since we need to make the browser request the newest files) Redirect any request to index.html so the SPA router can handle them You can see the complete config file here:\nserver { listen 80; server_name _; gzip on; gzip_types text/html text/css application/javascript; root /var/www/; index index.html; # Force all paths to load either itself (js files) or go through index.html. location /index.html { try_files $uri /index.html; add_header Cache-Control \u0026#34;no-store, no-cache, must-revalidate\u0026#34;; } location / { try_files $uri /index.html; expires 1y; add_header Cache-Control \u0026#34;public\u0026#34;; } } Building the final container The end result will be a combination of a) building the SPA in docker in the \u0026ldquo;build\u0026rdquo; step and then setting up a container from the nginx image and copying the JS from the build step as well as the checked in nginx config described above.\nFinally we expose port 80 and start nginx to serve the files.\nFROM node:12.6.0 AS build WORKDIR / COPY package.json package-lock.json tsconfig.json ./ RUN npm ci COPY ./src ./src COPY ./public ./public RUN npm run build --prod FROM nginx:1.16.1 COPY --from=build /build /var/www/ COPY ./k8s/config/nginx.conf /etc/nginx/conf.d/default.conf EXPOSE 80 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/build-a-progressive-web-app-in-docker-with-nginx-to-deploy-to-kubernetes-or-docker-swarm/","summary":"With my personal cloud setup based on kubernetes done (you can read about it here: https://heltweg.org/posts/run-a-personal-cloud-with-traefik-lets-encrypt-and-zookeeper/) it is time to actually deploy the first project into it.\nThe easiest application to deploy is a pure client side single page application, packaged in a docker container with a webserver like nginx to deliver the files. Packaging the application into it\u0026rsquo;s own container allows us to build a standardized container that can be run locally for testing or deployed to docker swarm and kubernetes.","title":"Build a progressive web app in docker with nginx to deploy to kubernetes or docker swarm"},{"content":"Kubernetes ingress with Traefik As mentioned in my last blog post I want to focus on a provider neutral setup for my own cloud, using technology that is not bound to any cloud offering whenever possible.\nWhile google cloud offers load balanced HTTP ingress by default it is apparently very expensive in comparison to running small nodes and I have heard only good things about using Traefik for kubernetes ingress.\nFor setting up Traefik I followed Manuel\u0026rsquo;s excellent guide with minor modifications (you can find the final files at the end of the article.)\nHTTPs and Let\u0026rsquo;s encrypt Traefik has built-in support for automatically getting and renewing HTTPS certificates with Let\u0026rsquo;s Encrypt. As HTTPS is good practice and a requirement for HTTP2 and PWAs anyway I set it up using example configurations from the Traefik docs.\nBecause I was using just one node for Traefik I chose to go with the easy setup of a local acme.json file that stores the certificate while the node is running.\nGKE Preemptible nodes, your own chaos monkey To save costs I chose to use \u0026ldquo;Preemtible VMs\u0026rdquo; as nodes to power my kubernetes cluster on GKE. According to google\u0026rsquo;s docs: \u0026ldquo;Preemptible VMs are Google Compute Engine VM instances that last a maximum of 24 hours and provide no availability guarantees.\u0026rdquo; This means the nodes in my kubernetes cluster randomly go down and are never up more than 24h. While this obviously is not a smart decision for a production setup I have chosen to embrace it and consider the nodes going down my own \u0026ldquo;chaos monkey\u0026rdquo; that forces me to write resilient code.\nA concrete example I ran into: The Let\u0026rsquo;s encrypt production API has a rate limit of requesting 5 certificates for the same URL in a week. Because my initial naive setup did not save the certificate anywhere it got lost whenever my Traefik node was terminated. While Traefik regenerates the certificate without any issue on startup\u0026hellip; after five startups I hit my rate limit and was greeted by an insecure warning without certificate.\nShared K/V store for Traefik with Zookeeper Enter a shared Key/Value store for Traefik. Using one is required if you want to run Traefik in cluster mode anyway (and I like to think my setup is easily scalable). It also means I can store my generated certificate in the K/V store where it will no longer just disappear when Traefik restarts.\nSince I have previous experience with Zookeeper and the setup was relatively painless I went with it.\nAll Kubernetes yaml files for the setup Finally the meat of the blog post, my complete setup as yaml files you can directly deploy into your GKE cluster:\nSet up Zookeeper first From this excellent ressource: https://github.com/kow3ns/kubernetes-zookeeper/blob/master/manifests/README.md\napiVersion: v1 kind: Service metadata: name: zk-hs labels: app: zk spec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: app: zk --- apiVersion: v1 kind: Service metadata: name: zk-cs labels: app: zk spec: ports: - port: 2181 name: client selector: app: zk --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: zk spec: serviceName: zk-hs replicas: 1 podManagementPolicy: Parallel updateStrategy: type: RollingUpdate template: metadata: labels: app: zk spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: \u0026#34;app\u0026#34; operator: In values: - zk topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: kubernetes-zookeeper imagePullPolicy: Always image: \u0026#34;gcr.io/google_containers/kubernetes-zookeeper:1.0-3.4.10\u0026#34; resources: requests: memory: \u0026#34;200M\u0026#34; cpu: \u0026#34;0.3\u0026#34; ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election command: - sh - -c - \u0026#34;start-zookeeper \\ --servers=1 \\ --data_dir=/var/lib/zookeeper/data \\ --data_log_dir=/var/lib/zookeeper/data/log \\ --conf_dir=/opt/zookeeper/conf \\ --client_port=2181 \\ --election_port=3888 \\ --server_port=2888 \\ --tick_time=2000 \\ --init_limit=10 \\ --sync_limit=5 \\ --heap=512M \\ --max_client_cnxns=60 \\ --snap_retain_count=3 \\ --purge_interval=12 \\ --max_session_timeout=40000 \\ --min_session_timeout=4000 \\ --log_level=INFO\u0026#34; readinessProbe: exec: command: - sh - -c - \u0026#34;zookeeper-ready 2181\u0026#34; initialDelaySeconds: 10 timeoutSeconds: 5 livenessProbe: exec: command: - sh - -c - \u0026#34;zookeeper-ready 2181\u0026#34; initialDelaySeconds: 10 timeoutSeconds: 5 volumeMounts: - name: datadir mountPath: /var/lib/zookeeper securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: - metadata: name: datadir spec: accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] resources: requests: storage: 5Gi Permissions for Traefik # create Traefik cluster role kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses verbs: - get - list - watch --- # create Traefik service account kind: ServiceAccount apiVersion: v1 metadata: name: traefik-ingress-controller namespace: default --- # bind role with service account kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: default Traefik config Note the configuration of zookeeper using the service address for the \u0026ldquo;client service\u0026rdquo; (cs) as well as the Let\u0026rsquo;s encrypt config here.\n# define Traefik configuration kind: ConfigMap apiVersion: v1 metadata: name: traefik-config data: traefik.toml: | # traefik.toml defaultEntryPoints = [\u0026#34;http\u0026#34;, \u0026#34;https\u0026#34;] [entryPoints] [entryPoints.http] address = \u0026#34;:80\u0026#34; [entryPoints.http.redirect] entryPoint = \u0026#34;https\u0026#34; [entryPoints.https] address = \u0026#34;:443\u0026#34; [entryPoints.https.tls] [zookeeper] endpoint = \u0026#34;zk-cs.default.svc.cluster.local:2181\u0026#34; watch = true prefix = \u0026#34;traefik\u0026#34; [acme] email = \u0026#34;your@email.com\u0026#34; storage = \u0026#34;traefik/acme/account\u0026#34; onHostRule = true caServer = \u0026#34;https://acme-v02.api.letsencrypt.org/directory\u0026#34; acmeLogging = true entryPoint = \u0026#34;https\u0026#34; [acme.httpChallenge] entryPoint = \u0026#34;http\u0026#34; [[acme.domains]] main = \u0026#34;your.domain.com\u0026#34; Deployment for Traefik I run just one replica in here to save costs in my dev setup but I\u0026rsquo;ve also scaled it up to three to test if it would stay up 100% of the time even with random nodes going down and everything works fine :).\n# declare Traefik deployment kind: Deployment apiVersion: extensions/v1beta1 metadata: name: traefik-ingress-controller spec: replicas: 1 template: metadata: labels: app: traefik-ingress-controller spec: serviceAccountName: traefik-ingress-controller terminationGracePeriodSeconds: 60 volumes: - name: config configMap: name: traefik-config containers: - name: traefik image: \u0026#34;traefik:1.7.14\u0026#34; volumeMounts: - mountPath: \u0026#34;/etc/traefik/config\u0026#34; name: config args: - --configfile=/etc/traefik/config/traefik.toml - --kubernetes - --logLevel=INFO Traefik service # Declare Traefik ingress service kind: Service apiVersion: v1 metadata: name: traefik-ingress-controller spec: selector: app: traefik-ingress-controller ports: - port: 80 name: http - port: 443 name: tls type: LoadBalancer Final result The final workloads with traefik and zookeeper\nAnd the kubernetes ingresses (ignore the app I used as demo for this).\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/run-a-personal-cloud-with-traefik-lets-encrypt-and-zookeeper/","summary":"Kubernetes ingress with Traefik As mentioned in my last blog post I want to focus on a provider neutral setup for my own cloud, using technology that is not bound to any cloud offering whenever possible.\nWhile google cloud offers load balanced HTTP ingress by default it is apparently very expensive in comparison to running small nodes and I have heard only good things about using Traefik for kubernetes ingress.","title":"Run a personal Cloud with Traefik, Lets encrypt and Zookeeper"},{"content":"Why invest in a personal cloud It has never been easier to host your personal side projects. Tools like surge.sh or Heroku make it painless to run your code. And if all else fails the old reliable \u0026ldquo;drag and drop files to a ftp\u0026rdquo; is always there - so why invest time into setting up your own personal cloud with kubernetes?\nMy goal for technology is typically to find a setup that gets boring to work with because I know it well and can focus on delivering new functionality. For that a setup needs to be future proof (so it continues to work for a long time), generic (so I can use it for a wide range of applications and do not need to switch for every project) and not bound to any company or product.\nDocker \u0026amp; kubernetes Docker and Kubernetes check most of these boxes. Kubernetes has de-facto won the orchestration war for containerized applications and managed kubernetes offerings from all major cloud providers means there is no provider lock-in. As an open source project that is not bound to individual company or setup you retain flexibility. Lastly learning more about it is useful in any case - if you stop developing your side projects the devops knowledge you gained still looks good on a CV.\nA further important point that separates kubernetes from similar offerings is that you can 100% forget about hardware while still being free to use standard technology. That means you do not need to maintain a set of servers if you use a managed kubernetes offering (like you would need to with docker swarm) but you are still not locked into any provider (like you would be by using AWS Beanstalk, Firebase or Heroku).\nWhy not\u0026hellip;? My own personal journey has made me try out the following alternatives in the order they are written down here and I have abandoned them all.\nOwn servers You need to maintain and update your own servers. If you do it without automation you will forget what you did. The complexity and learning curve is close to kubernetes anyway. Instead of learning about deployments and stateful sets you will need to know about processes or package managers. Scaling is harder. Services (e.g. Firebase) Very easy to set up and use, probably advisable if you work on one or a few projects Using vendor specific tools (like firebase realtime database) locks you and the logic of your app into that vendor. What if they change their offering (price? functionality?) Services are the hammer that makes very problem look like a nail. Instead of picking the best technology for a job you will start trying to shoehorn your problems to be solved by what is available. Docker Swarm I really liked it, very close to kubernetes but considerably easier Sadly still forces you to manage your own servers to set up the swarm cluster, I could not find a \u0026ldquo;managed docker swarm\u0026rdquo; solution (if you know one, let me know!) The value of infrastructure as code By choosing kubernetes you also commit to keeping your infrastructure in code which has a plethora of benefits:\nit automatically and correctly documents your infrastructure and any changes you made by reading git logs you can easily redeploy it on new providers or e.g. locally for development all your infrastructure is in one place so you don\u0026rsquo;t need to think about how each project solved a problem Organizing your projects For my personal cloud I chose:\nA Google Cloud Kubernetes Engine Cluster as cloud Traefik as ingress router to forward requests to my projects Zookeeper for shared state between Traefik nodes Let\u0026rsquo;s Encrypt to automatically set up HTTPS Every project has it\u0026rsquo;s own Google Cloud Container Registry as a private docker repository Service accounts allow the GKE cloud to pull the docker images of individual projects from the respective repository Infrastructure descriptions for the generic GKE setup and services is in one place, infrastructure descriptions for the individual projects is in their respective code Deployment is handled with gitlab Further reading Read my blog post about the actual setup of this cloud: /posts/run-a-personal-cloud-with-traefik-lets-encrypt-and-zookeeper. Also read this helpful article: Traefik on a Google Kubernetes Engine Cluster managed by Terraform by Manuel Zapf About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/kubernetes-for-sideprojects-hardware-is-dead/","summary":"Why invest in a personal cloud It has never been easier to host your personal side projects. Tools like surge.sh or Heroku make it painless to run your code. And if all else fails the old reliable \u0026ldquo;drag and drop files to a ftp\u0026rdquo; is always there - so why invest time into setting up your own personal cloud with kubernetes?\nMy goal for technology is typically to find a setup that gets boring to work with because I know it well and can focus on delivering new functionality.","title":"Kubernetes for sideprojects: Hardware is dead"},{"content":"Entity-Systems for typescript based games For my latest game project Frozzen I want to explore how an external UI, build with Angular, would work for a browser based game. Since Angular is written in Typescript that means ideally the game should also use the same.\nI have used Artemis ODB as framework for a Java based game in the past and liked it a lot. Entity-Systems are much better introduced by any of the huge amount of articles out there (for example the classic on T=Machine but I feel they are especially well suited to Javascript/Typescript development.\nIf you work with a strict separation of logic into systems and data only into components there is a very natural way to serialize components, JSON. Whole levels can be expressed as an array of JSON data that is used to set up components. That is why I prefer a very basic but strict implementation like artemis over similar frameworks like PhaserJS.\nI started my development with artemists, a Typescript port of artemis by darkoverlordofdata. Unfortunately the code is a bit outdated and does not use import/export and can not be directly imported for newer Typescript versions (since it extends the built-in Array).\nWith darkoverlordofdata’s permission I did a quick update to the Typescript parts of the code only, adding import/export support and fixing the build for newer Typescript versions. You can find the updated version here. If you are looking for an example of that framework in action you can play an example level of Frozzen here.\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/entity-systems-for-typescript-based-games/","summary":"Entity-Systems for typescript based games For my latest game project Frozzen I want to explore how an external UI, build with Angular, would work for a browser based game. Since Angular is written in Typescript that means ideally the game should also use the same.\nI have used Artemis ODB as framework for a Java based game in the past and liked it a lot. Entity-Systems are much better introduced by any of the huge amount of articles out there (for example the classic on T=Machine but I feel they are especially well suited to Javascript/Typescript development.","title":"Entity-Systems for typescript based games"},{"content":"Blog requirements I recently evaluated what how writing content and carving out a little personal space on the internet would look like for me and came up with a list of requirements:\nOwn my content (No third party platforms as only distribution method. There was a time before medium and there will be a time after medium.) Easy to set up and maintain (I don\u0026rsquo;t want to set up and host databases or complicated CMS systems and when I come back to this in a month it should be easy to update.) Long term secure storage (By not hosting my content with a third party but in git I can easily back it up, keep old revisions and there is no worry of the hosting provider going down.) All these requirements are solved by using a static site generator and keeping the original files versioned in git. When setting up some for of continuous integration (for example with gitlab) it should be reasonably easy to maintaining for a long time.\nI chose gohugo as a site generator due to the excellent setup article written by Fabian Gruber here: https://www.fabiangruber.de/posts/setup-and-deployment\nAutomated builds I set up an automated build using gitlab using the following gitlab-ci.yml:\nstages: - build build: image: monachus/hugo:v0.55.3 stage: build script: - hugo artifacts: paths: - public/ This uses a docker image for Hugo and builds the static page from the sources provided.\nContinuous delivery I chose firebase to host the website because I have previous experience with it. It is very easy to create and deploy a static website as well as wire it up with a HTTPS domain.\nAfter creating a new project in firebase you\u0026rsquo;ll need to get a token that allows gitlab to make deployments on your behalf:\nfirebase login:ci Set it up as a secret variable in gitlab called \u0026ldquo;FIREBASE_TOKEN\u0026rdquo;.\nTo add firebase to the gohugo project navigate to the directory in the CLI can call:\nfirebase init Armed with the token that allows gitlab to deploy in your name as well as a configured firebase project we can add the last step to the gitlab-ci file - deployment.\nYou can see the full gitlab-ci.yml file here:\nstages: - build - deploy build: image: monachus/hugo:v0.55.3 stage: build script: - hugo artifacts: paths: - public/ deploy: image: devillex/docker-firebase:slim stage: deploy only: - master script: - firebase use \u0026lt;project-name\u0026gt; --token $FIREBASE_TOKEN - firebase deploy --only hosting -m \u0026#34;Pipe $CI_PIPELINE_ID Build $CI_BUILD_ID\u0026#34; --token $FIREBASE_TOKEN environment: name: production url: https://\u0026lt;project-name\u0026gt;.firebaseapp.com dependencies: - build About Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/deploy-a-personal-blog-with-hugo-firebase-and-gitlab/","summary":"Blog requirements I recently evaluated what how writing content and carving out a little personal space on the internet would look like for me and came up with a list of requirements:\nOwn my content (No third party platforms as only distribution method. There was a time before medium and there will be a time after medium.) Easy to set up and maintain (I don\u0026rsquo;t want to set up and host databases or complicated CMS systems and when I come back to this in a month it should be easy to update.","title":"Deploy a personal blog with Hugo, Firebase and Gitlab"},{"content":" Meal prepping is the practice of preparing multiple meals at once and either freezing them or keeping them in the fridge to eat over the upcoming days. There is an active community on reddit (r/MealPrepSunday/) and various blogs online.\nOften mealprep containers are separated in three sections for a carb source, a protein source and vegetables.\nWith mealpreplist I wanted to save my own, simple meal components for each of these sections and allow myself an easy way of combining them to a meal to prepare.\nBecause I felt easy filtering of recipes and various other functions might have worth for other people to I decided to built the product with user accounts and the ability to pay for a premium account with additional features.\nTechnology Google Firebase With mealpreplist I wanted to allow users to submit their own recipes, rate and comment on them as well as buy premium accounts for additional functionality with Stripe. All of this would normally mean a fairly complex backend but by relying on Google\u0026rsquo;s firebase I was able to implement everything without my own servers.\nI used Firestore to store recipe and ingredient data as well as user comments Firebase Storage allowed me to let users upload pictures of their recipes Firebase User Accounts/Login was used for user management Firebase Cloud Functions handle the interaction with Stripe and update user\u0026rsquo;s premium status Frontend Mealpreplist was my personal project to get experience in React development so the frontend is written in React using Typescript. During development I built the UI with react material ui though I later switched to some custom built components.\nThe data flow is based on redux using react-redux and redux-saga for data loading and other side effects like analytics tracking.\nI made sure to make the app mobile friendly from the start, relying on flexbox to shrink and expand the display depending on screen size and media queries to hide not needed information on smaller screens.\nCI CI runs on gitlab, building every commit and allowing me to have an automated deploy to Firebase hosting.\nMarketing For marketing I relied on building an email list using mailchimp and posting to the appropriate subreddit. The feedback was very good and the spike in visitors allowed me to gather a lot of email list subscribers but converted incredibly poorly (only one subscription from 6000 visitors).\nI kept up sending out regular email updates about development (mostly regarding feature requests from the reddit feedback) during the time spend on the project but never saw any improvement in subscriptions or visitor numbers.\nWithout any continued marketing mealpreplist still gets a low number of regular visitors that use it for the recipes.\nConclusion I mainly built mealpreplist to save recipes for myself and to learn React and I am happy with the outcome for both. I learned a lot about the kind of products I want to build from it (e.g. I don\u0026rsquo;t want to be in a business were I need to create a lot of content myself) and I also cemented what I learned from Whatchr regarding being more sure about the concrete value proposition of a product before trying to sell it.\n","permalink":"https://heltweg.org/posts/mealpreplist-dot-com-your-weekly-mealprep-made-easy/","summary":"Meal prepping is the practice of preparing multiple meals at once and either freezing them or keeping them in the fridge to eat over the upcoming days. There is an active community on reddit (r/MealPrepSunday/) and various blogs online.\nOften mealprep containers are separated in three sections for a carb source, a protein source and vegetables.\nWith mealpreplist I wanted to save my own, simple meal components for each of these sections and allow myself an easy way of combining them to a meal to prepare.","title":"mealpreplist.com - Your weekly mealprep, made easy"},{"content":"Whatchr was born of a personal problem. I regularly watch let\u0026rsquo;s play content on youtube (following a group of content creators with their series), mainly following the channels of The Yogscast.\nYoutube\u0026rsquo;s focus is on either suggesting content it thinks you find interesting on your \u0026ldquo;Suggestions\u0026rdquo; or content divided in daily chunks in your \u0026ldquo;Feed\u0026rdquo;. There is no nice way to watch multiple series other than bookmarking playlists with watch positions.\nI set out to change that for myself, building a small \u0026ldquo;Netflix for Youtube\u0026rdquo;.\nOriginally I had also planned to tag videos with the content creator appearing in them so you can follow your favorite content creator over multiple channels.\nTechnology Watchr would need to solve a few typical web app problems: Managing and displaying data and allow users to create accounts and log in (to save their individual watch positions). In addition - since all data about playlists and videos came from the youtube API - I also needed to build a caching layer to speed up response time and not run into API limits.\nThe frontend - Angular with Typescript, SCSS, HTML At the time of creation Angular was releasing Beta versions. Since I had previous experience with AngularJS and Typescript I chose Watchr as personal learning project for the new stack of Angular and Observables with RXJS.\nWhile I build all features and a basic material design the talented Simon Brix helped me with the final design and did most of the implementation of it in SCSS/HTML. Thanks!\nThe API - NodeJS with express/Typescript, PostgreSQL With the goal of staying in a unified language stack I decided to write the API in NodeJS (using Typescript) with the express framework.\nAn initial version of the App used CouchDB as a database but the added overhead of writing views for every data selection seemed tedious. During development I switched to PostgreSQL and their JSON support. It allowed for fast development (since I am familiar with PostgreSQL), is performant and allows for easy query building due to the built-in JSON operators.\nFor user accounts I decided to implement Google login using Passport. That allowed me to not directly save any user data (always a good idea to save as little important data as possible) but the user id and name.\nThe Import - NodeJS with Typescript, PostgreSQL, Youtube API, Cronjobs Importing new data would need to happen regularly to keep the database up to date. While the youtube API is very good I needed to focus on a manually curated set of channels to import. I therefor decided on writing a small script that could be called by a cronjob and import all playlists and videos of a specific channel.\nThe script was written in Typescript (again, keeping the same techstack) and used the same database as the API to save JSON objects directly from the youtube API. I used a combination of observables (with RXJS) and functional programming to make the import code easily manageable while being very efficient,\nHosting - DigitalOcean When everything was done I implemented deployment on a digital ocean droplet using gitlab CI. I disabled any log in on the droplet but public key authentication, configured a private key as CI variable in gitlab and set it up during the build job. After the project was build gitlab rsynced the files to the droplet and restarted the node process.\nI configured the import cronjobs manually with a channel id for each job. The database was hosted on the same droplet and the whole server backed up automatically with daily snapshots.\nIf I would redo this project I would definitely improve the deployment workflow so I do not need to manually configure cronjobs as getting back to the knowledge how and where to do that was one of the main pain points of the project.\nMarketing I posted once to the respective reddit https://www.reddit.com/r/Yogscast/comments/5tdtio/follow_yogscast_playthroughs_easier_give_me_your/ but did not do any marketing otherwise.\nNot surprisingly I was the only active user for the time Whatchr was alive.\nConclusion Whatchr was a fun project and allowed me to learn a big set of technologies while scratching my own itch. I learned a lot about the youtube API and the internal organisation of their data.\nOverall the project gave me an appreciation for better planning of a value proposition beforehand (because I regularly had problems articulating what problem it solves) and marketing.\nAbout Me I love getting emails! Talk to me at philip@heltweg.org! ","permalink":"https://heltweg.org/posts/whatchr-dot-com-a-better-youtube-playlist-experience/","summary":"Whatchr was born of a personal problem. I regularly watch let\u0026rsquo;s play content on youtube (following a group of content creators with their series), mainly following the channels of The Yogscast.\nYoutube\u0026rsquo;s focus is on either suggesting content it thinks you find interesting on your \u0026ldquo;Suggestions\u0026rdquo; or content divided in daily chunks in your \u0026ldquo;Feed\u0026rdquo;. There is no nice way to watch multiple series other than bookmarking playlists with watch positions.","title":"Whatchr.com - a better youtube playlist experience"},{"content":"Gameplay Screenshots Login Menu Select a scenario Select a party Manage your party Manage party member and test level Gameplay Old History ","permalink":"https://heltweg.org/posts/frozzen/","summary":"Gameplay Screenshots Login Menu Select a scenario Select a party Manage your party Manage party member and test level Gameplay Old History ","title":"Frozzen - a turn-based RPG"}]